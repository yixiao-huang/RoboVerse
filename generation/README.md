# EmbodiedGen for 3D Asset and Interactive Scene Generation

[![ğŸ“– Documentation](https://img.shields.io/badge/ğŸ“–-Documentation-blue)](https://horizonrobotics.github.io/EmbodiedGen/)
[![GitHub](https://img.shields.io/badge/GitHub-EmbodiedGen-black?logo=github)](https://github.com/HorizonRobotics/EmbodiedGen)
[![ğŸ“„ arXiv](https://img.shields.io/badge/ğŸ“„-arXiv-b31b1b)](https://arxiv.org/abs/2506.10600)
[![ğŸ¥ Video](https://img.shields.io/badge/ğŸ¥-Video-red)](https://www.youtube.com/watch?v=rG4odybuJRk)
[![ä¸­æ–‡ä»‹ç»](https://img.shields.io/badge/ä¸­æ–‡ä»‹ç»-07C160?logo=wechat&logoColor=white)](https://mp.weixin.qq.com/s/HH1cPBhK2xcDbyCK4BBTbw)

[![ğŸ¤— Asset Gallery](https://img.shields.io/badge/ğŸ¤—-EmbodiedGen_Asset_Gallery-blue)](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Gallery-Explorer)
[![ğŸ¤— Image-to-3D Demo](https://img.shields.io/badge/ğŸ¤—-Image_to_3D_Demo-blue)](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Image-to-3D)
[![ğŸ¤— Text-to-3D Demo](https://img.shields.io/badge/ğŸ¤—-Text_to_3D_Demo-blue)](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Text-to-3D)
[![ğŸ¤— Texture Generation Demo](https://img.shields.io/badge/ğŸ¤—-Texture_Gen_Demo-blue)](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Texture-Gen)

---

> ğŸš€ **EmbodiedGen** provides a unified framework for generating **sim-ready 3D assets** and **interactive 3D scenes**, fully compatible with RoboVerse and multiple popular simulators.

We use [EmbodiedGen](https://horizonrobotics.github.io/EmbodiedGen) as the foundation platform for generating realistic, physically consistent, and simulation-ready 3D contents.  
You can seamlessly import generated assets into any RoboVerse simulator following these tutorials:
- [Import Assets](https://roboverse.wiki/metasim/get_started/quick_start/14_real_asset)
- [Import 3D Scene](https://roboverse.wiki/metasim/get_started/quick_start/16_embodiedgen_layout)
- [Import 3DGS Background](https://roboverse.wiki/metasim/get_started/quick_start/15_gs_background)

Explore the [EmbodiedGen Asset Gallery](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Gallery-Explorer) to browse and use generated sim-ready assets and follow [Any Simulators Tutorial](#download-assets). To generate diverse sim-ready 3D assets and interactive 3D scenes by yourself, please [install EmbodiedGen](https://horizonrobotics.github.io/EmbodiedGen/install).


## ğŸ§­ Overview

<img src="assets/overall.jpg" alt="Overall Framework" width="700"/>


<h2 id="image-to-3d">ğŸ–¼ï¸ Image-to-3D</h2>

[![ğŸ¤— Hugging Face](https://img.shields.io/badge/ğŸ¤—-Image_to_3D_Demo-blue)](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Image-to-3D)

Generate physically plausible 3D asset URDF from single input image, offering high-quality support for digital twin systems.
(HF space is a simplified demonstration. For the full functionality, please refer to [img3d-cli](https://horizonrobotics.github.io/EmbodiedGen/tutorials/image_to_3d)). Use generated 3D assets in RoboVerse, see [Import Assets Tutorial](https://roboverse.wiki/metasim/get_started/quick_start/14_real_asset).

```sh
img3d-cli --image_path apps/assets/example_image/sample_00.jpg apps/assets/example_image/sample_01.jpg apps/assets/example_image/sample_19.jpg \
--n_retry 1 --output_root outputs/imageto3d

# See result(.urdf/mesh.obj/mesh.glb/gs.ply) in ${output_root}/sample_xx/result
```

<h2 id="text-to-3d">ğŸ“ Text-to-3D</h2>

[![ğŸ¤— Hugging Face](https://img.shields.io/badge/ğŸ¤—-Text_to_3D_Demo-blue)](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Text-to-3D) 

Create 3D assets from text descriptions for a wide range of geometry and styles. (HF space is a simplified demonstration. For the full functionality, please refer to [text3d-cli](https://horizonrobotics.github.io/EmbodiedGen/tutorials/text_to_3d)). Use generated 3D assets in RoboVerse, see [Import Assets Tutorial](https://roboverse.wiki/metasim/get_started/quick_start/14_real_asset).

```sh
text3d-cli --prompts "small bronze figurine of a lion" "A globe with wooden base" "wooden table with embroidery" \
    --n_image_retry 1 --n_asset_retry 1 --n_pipe_retry 1 --seed_img 0 \
    --output_root outputs/textto3d
```


<h2 id="texture-generation">ğŸ¨ Texture Generation</h2>

[![ğŸ¤— Hugging Face](https://img.shields.io/badge/ğŸ¤—-Texture_Gen_Demo-blue)](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Texture-Gen)

Generate visually rich textures for 3D mesh, detailed guide: [Texture Editing Tutorial](https://horizonrobotics.github.io/EmbodiedGen/tutorials/texture_edit/).

```sh
texture-cli --mesh_path "apps/assets/example_texture/meshes/robot_text.obj" \
"apps/assets/example_texture/meshes/horse.obj" \
--prompt "ä¸¾ç€ç‰Œå­çš„å†™å®é£æ ¼æœºå™¨äººï¼Œå¤§çœ¼ç›ï¼Œç‰Œå­ä¸Šå†™ç€â€œHelloâ€çš„æ–‡å­—" \
"A gray horse head with flying mane and brown eyes" \
--output_root "outputs/texture_gen" \
--seed 0
```

<h2 id="3d-scene-generation">ğŸŒ 3D Scene Generation</h2>

Automatically generate background scenes (color mesh + 3D Gaussian Splatting) from text prompts. Typical runtime: ~30 minutes per scene. Details: [Scene Generation Generation Tutorial](https://horizonrobotics.github.io/EmbodiedGen/tutorials/scene_gen), [Import 3DGS Background](https://roboverse.wiki/metasim/get_started/quick_start/15_gs_background).

```sh
CUDA_VISIBLE_DEVICES=0 scene3d-cli \
--prompts "Art studio with easel and canvas" \
--output_dir outputs/bg_scenes/ \
--seed 0 \
--gs3d.max_steps 4000 \
--disable_pano_check
```

<h2 id="layout-generation">ğŸï¸ Layout(Interactive 3D Worlds) Generation</h2>

Generating one interactive 3D scene from task description with [Layout Generation Tutorial](https://horizonrobotics.github.io/EmbodiedGen/tutorials/layout_gen/) takes approximately 30 minutes. Use generated layout in RoboVerse, see [Import 3D Scene](https://roboverse.wiki/metasim/get_started/quick_start/16_embodiedgen_layout).

```sh
layout-cli --task_descs "Place the pen in the mug on the desk" "Put the fruit on the table on the plate" \
--bg_list "outputs/bg_scenes/scene_list.txt" --output_root "outputs/layouts_gen" --insert_robot
```

<h2 id="any-simulators">ğŸ® Any Simulators</h2>

Use EmbodiedGen-generated assets with correct physical collisions and consistent visual effects in any simulator, see [Any Simulator Tutorial](https://horizonrobotics.github.io/EmbodiedGen/tutorials/any_simulators).

| Simulator | Conversion Class |
|-----------|------------------|
| [isaacsim](https://github.com/isaac-sim/IsaacSim) | MeshtoUSDConverter |
| [mujoco](https://github.com/google-deepmind/mujoco) / [genesis](https://github.com/Genesis-Embodied-AI/Genesis) | MeshtoMJCFConverter |
| [sapien](https://github.com/haosulab/SAPIEN) / [isaacgym](https://github.com/isaac-sim/IsaacGymEnvs) / [pybullet](https://github.com/bulletphysics/bullet3) | EmbodiedGen generated .urdf can be used directly |


<h3 id="download-assets">Download Sim-ready Assets</h3>

Select your favorite sim-ready assets from the [EmbodiedGen Asset Gallery](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Gallery-Explorer), and automatically convert to `.urdf`, `.usd`, `.mjcf` for use in any simulator.

```sh
python generation/download_asset.py \
    --target_type "dataset/basic_furniture/table" \
    --uuid "*" \
    --download_num 5
```

Arguments:
- `--target_type`: Specify the asset category. See `Primary Category` and `Secondary Category` in [EmbodiedGen Asset Gallery](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Gallery-Explorer).
- `--uuid`: Set `uuid="*"` to download all assets in the `target_type`, or specify uuid in `Asset Details` in [EmbodiedGen Asset Gallery](https://huggingface.co/spaces/HorizonRobotics/EmbodiedGen-Gallery-Explorer), e.g. `710fc1c383f5542d8f52822fc08ee66c`, `16b602cfd8285027af606f20aaadc739`.
- `--download_num`: Number of assets to download. Default is None to download all assets in `target_type`.

## ğŸ“š Citation

If you use EmbodiedGen in your research or projects, please cite:

```bibtex
@misc{wang2025embodiedgengenerative3dworld,
      title={EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence},
      author={Xinjie Wang and Liu Liu and Yu Cao and Ruiqi Wu and Wenkang Qin and Dehui Wang and Wei Sui and Zhizhong Su},
      year={2025},
      eprint={2506.10600},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2506.10600},
}
```
